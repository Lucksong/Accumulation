# 13.基于ELK方案使用flink对linux服务器日志进行信息提取

---

**1 基本思路**

--1.1 filebeat对服务器日志采集输出到logstash

--1.2 在logstash中对日志进行grok过滤提取信息并输出到kafka

--1.3 使用flink读取kafka中的数据并写入elasticsearch，并在kibana中展示


**2 filebeat对服务器日志采集输出到logstash**

--2.1 filebeat配置在10.10.27.32，filebeat.yml配置如下
~~~
    filebeat.inputs:
    - type: log
      enabled: true
      paths:
         - /var/log/syslog
      fields:
          path_name: syslog
      fields_under_root: true
    
    - type: log
      enabled: true
      paths:
        - /var/log/nova/*.log
      fields: 
          path_name: nova 
      fields_under_root: true
    
    
    filebeat.config.modules:
      path: ${path.config}/modules.d/*.yml
      reload.enabled: false
    
    
    setup.template.settings:
      index.number_of_shards: 1
    
    
    setup.kibana:
    
    
    output.logstash:
      hosts: ["10.10.26.236:5044"]
    
    
    processors:
      - add_locale: ~
      - add_host_metadata: 
          netinfo.enabled: true
      - add_cloud_metadata: ~
    
      - drop_fields:
         when:
            has_fields: ['host.mac']
         fields: ["host.mac"]
~~~

--2.2 配置说明

- filebeat.inputs配置输入文件(日志)的路径，type表示文件类型，paths可配置多个日志路径
- 在本例中，由于在logstash中需要对日志来源进行区分从而选择对应的grok正则表达式进行过滤，所以对每个输入路径添加了一个path_name并置于根域
~~~
    fields：向输出的每一条日志添加额外的信息，比如“path_name: nova”，方便后续对日志进行分组统计。默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录。
        fields: 
            path_name: nova 
              
    fields_under_root：如果该选项设置为true，则新增fields成为顶级目录，而不是将其放在fields目录下。自定义的field会覆盖filebeat默认的field。
        fields: 
            path_name: nova  
        fields_under_root: true
~~~
- output.logstash配置输出到logstash，分别为logstash(10.10.26.236)的安装地址和端口(5044)，除此之外，filebeat可以直接输出到kafka、elasticsearch等


**3 在logstash中对日志进行grok过滤提取信息并输出到kafka**

--3.1 logstash安装在10.10.26.236虚拟机上，/logstash/conf.d的t3.conf配置如下(logstash此时的启动命令:nohup /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/t3.conf&)
~~~
    input {
        beats {
            port => "5044"
        }
    }
    filter {
       if ( [path_name] =~ "nova" ) {
    	grok {
                match => {
                   "message" => "%{TIMESTAMP_ISO8601:timestamp} %{NUMBER:pid} %{LOGLEVEL:loglevel}[\s]+(?<message_content>.+)"
    	       #"message" => "%{TIMESTAMP_ISO8601:timestamp} %{NUMBER:pid} %{LOGLEVEL:loglevel} %{JAVACLASS:class}[\s]+%{GREEDYDATA:message_content}"
                }
    
         	}
       } else if ( [path_name] =~ "syslog" ) {
            grok {
                match => {
                   #"message" => "%{TIMESTAMP_ISO8601:timestamp} %{NUMBER:pid} %{LOGLEVEL:loglevel} %{JAVACLASS:class}\s+(?<message_content>.+)"
                    "message" => "%{TIMESTAMP_ISO8601:timestamp}[\s]+(?<ip>[\w]+)[\s]+(?<pid>[\w]+)[\s]+(?<log>[\w]+)[\s]+%{LOGLEVEL:loglevel}[\s]+%{GREEDYDATA:message_content}"
                }
    
            }
       }
    }
    output {
    
      if ( "_grokparsefailure" in [tags]  ) {
           kafka {
               bootstrap_servers => "compute236:6667,compute237:6667,compute238:6667"
           	   topic_id => 'logstash_fail'
               codec => json
           }	
      } else if ( [path_name] =~ "nova"  ) {
          kafka {
            bootstrap_servers => "compute236:6667,compute237:6667,compute238:6667"
            topic_id => 'logstash_nova'
            codec => json
         } 
      } else if ( [path_name] =~ "syslog"  ) {
          kafka {
            bootstrap_servers => "compute236:6667,compute237:6667,compute238:6667"
            topic_id => 'logstash_syslog'
            codec => json
         }
      }
      
    }
~~~

--3.2 配置说明

- input表示logstash的输入，此时该输出即为2中filebeat的输出
- filter对input的文件进行过滤，此时对不同文件进行过滤的条件依据即为2中添加的path_name域的值
- filter的正则表达式写法可参考12.logstash的grok日志过滤正则表达式，提取出来的信息自动添加到域中
- ouput输出到kafka中，且不同日志输出到不同topic中，filter进行grok匹配失败的那条消息的tags中会加入_grokparsefailure值，可据此判断filter是否grok成功


**4 使用flink读取kafka中的数据并写入elasticsearch，并在kibana中展示**

--4.1 当过滤提取后的数据输出到kafka的时候，使用flink从kafka中获取对应的数据，并写入elasticsearch，如果后续要对对应信息处理，可以在使用flink处理之后再写入elasticsearch
--4.2 以/var/log/nova目录下的日志消息写入elasticsearch的NovaMain函数为例
~~~
    public class NovaMain {
    
        public static void main(String[] args) throws Exception{
            final StreamExecutionEnvironment env = EnvironmentInitiator.initEnv(5 * 60, args);
            try{
                // 加载kafka和elasticsearch的配置
                Map<String, String> p = env.getConfig().getGlobalJobParameters().toMap();
                FMConfig config = new FMConfig();
                LogstashNovaConfig lnc = config.loadModuel(LogstashNovaConfig.PREFIX, p, LogstashNovaConfig.class);
    
                // 从kafka中获取数据并转化数据格式
                KafkaReader<NovaLogRecord> reader = new KafkaReader<>();
                DataStream<NovaLogRecord> baseFlow = reader.readDataFromKafka(env, lnc.getKafkaConfig(), "log-group",
                        new LogStringToNovaRecordMapper("10.10."), "kafka-reader");
    
                // 将数据写入elasticsearch
                Map<String, Object> schema = LogSchemaUtils.buildNovaLogMapping();
                ElasticSearchIndexUtil.initESIndex(
                        lnc.getElasticSearchConfig()
                        , schema);
    
                baseFlow.addSink(
                        new TimestampedRecordElasticSearchSink(lnc.getElasticSearchConfig(), schema, null)
                );
                env.execute(LogstashNovaConfig.PREFIX);
    
            }catch (Exception e){
                e.printStackTrace();
                System.err.println("start program error. FlowMatrix --zookeeper <zookeeperAdress> --config <configpath>" +
                        " --name <jobName> --interval <intervalInMinute> --indexName <indexName>");
                System.err.println(e.toString());
                return;
            }
        }
    
    }
~~~


**5 参考**

- 5.1 filebeat.yml配置参考：https://blog.csdn.net/d1240673769/article/details/95476697
- 5.2 filebeat介绍：https://www.jianshu.com/p/0a5acf831409
- 5.3 elk介绍：https://www.ibm.com/developerworks/cn/opensource/os-cn-elk-filebeat/index.html
- 5.4 logstash的安装和配置参考：https://www.cnblogs.com/cpy-devops/p/9287531.html
- 5.5 logstash的安装和使用：https://www.cnblogs.com/tonglin0325/p/9044674.html
- 5.6 logstash的参考：https://segmentfault.com/a/1190000016590936#articleHeader3
- 5.7 logstash输出到kafka：https://blog.csdn.net/yelllowcong/article/details/80848515