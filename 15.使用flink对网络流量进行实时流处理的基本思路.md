# 15.使用flink对网络流量进行实时流处理的基本思路

---

**1 项目中实际使用flink处理网络流量的思路**

--1.1 Flink的程序主要由三部分构成，分别为Source、Transformation、Sink。DataSource主要负责数据的读取，Transformation主要
负责对属于的转换操作，Sink负责最终数据的输出
- Source：不管是对网络流量进行实时流处理，还是对服务器日志进行分析，在项目的实际使用中，都是事先将实时数据写入kafka，然
后flink以kafka为source数据源读取数据
- Transformation：涉及到flink提供的各种stream api的使用
- Sink：在项目的实际使用中，都是将经过flink转换操作的实时数据写入elasticsearch，然后kibana展示，也可以promethues配置
elasticsearch的数据源在监控前台展示

--1.2 Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，它能够基于同一个Flink运行时(Flink Runtime)，提
供支持流处理和批处理两种类型应用的功能
- 最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process 
Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据
流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算
- 大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs）进行编程，比如DataStream API（有界或无界流数据）以及
DataSet API（有界数据集）。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），
连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代
- Table API 以表为中心，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构
（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。除此之外，
Table API程序在执行之前会经过内置优化器进行优化，你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 
DataStream 以及 DataSet 混合使用
- Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。
SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。

**2 Source：flink从kafka读取数据**

--2.1 项目中使用flink主要是作为kafka的消费者，将从kafka中读取数据的操作封装为KafkaReader(以计算租户流量的top-n的TopUser
函数为例)
- flink的main函数调用KafkaReader读取指定
~~~
    // 从zookeeper和flink命令行参数读取kafka的配置
    Map<String, String> p = env.getConfig().getGlobalJobParameters().toMap();
    FMConfig config = new FMConfig();
    TopUserConfig tuc = config.loadModuel(TopUserConfig.PREFIX, p, TopUserConfig.class);
    // 根据拿到的kafka配置(kafkade urlsh和topic)的读取
    KafkaReader<SFlowRecord> reader = new KafkaReader<>();
    DataStream<SFlowRecord> baseFlow = reader.readDataFromKafka(env, tuc.getKafkaConfig(), "flink-group",
            new SFlowJson2FlowRecordMap(), "kafka-reader");
~~~

- KafkaReader的实现(封装了FlinkKafkaConsumer)
~~~
    public class KafkaReader<T> {
        /***
         * Read data from kafka and add watermark every 10 secondes
         * @param env global environment
         * @param kafkaConfig kafka config data structure;
         * @param groupId kafka group id
         * @param mapper how to transfer a message from kafka to a T
         * @param name name of mapper
         * @return a new DataStream of T
         */
        public DataStream<T> readDataFromKafka(StreamExecutionEnvironment env,
                                               KafkaConfig kafkaConfig,
                                               String groupId,
                                               FlatMapFunction<String, T> mapper,
                                               String name) {
    
          return readDataFromKafka(env,kafkaConfig.getUrls(),groupId,kafkaConfig.getTopic(),mapper,name);
        }
    
        /***
         * Read data from kafka and add watermark every 10 secondes
         * @param env global environment
         * @param kafkaUrl kafka brokers url compute236:6667,compute237:6667,compute238:6667
         * @param groupId kafka group id
         * @param topic what topic should read from kafka
         * @param mapper how to transfer a message from kafka to a T
         * @param name name of mapper
         * @return a new DataStream of T
         */
        public DataStream<T> readDataFromKafka(StreamExecutionEnvironment env,
                                               String kafkaUrl,
                                               String groupId,
                                               String topic,
                                               FlatMapFunction<String, T> mapper,
                                               String name) {
    
            Properties props = new Properties();
            props.setProperty("bootstrap.servers", kafkaUrl);
            props.setProperty("group.id", groupId);
            // 第一个参数为kafka的topic，props包含了kafka的url
            FlinkKafkaConsumer010<String> consumer =
                    new FlinkKafkaConsumer010<>(topic, new SimpleStringSchema(), props);
            
            // env.addSource()表示flink从kafka获取数据
            DataStream<String> kafkaInput;
            if (name != null)
                kafkaInput = env.addSource(consumer).name(name);
            else
                kafkaInput = env.addSource(consumer);
                
            // 处理数据(将json字符串映射为javaBean类)
            // assignTimestampsAndWatermarks分配timestamp和生成watermark
            SingleOutputStreamOperator baseFlow =
                    ((DataStreamSource<String>) kafkaInput)
                            .setParallelism(1)
                            .rebalance()
                            .flatMap(mapper)
                            .name(name)
                            .assignTimestampsAndWatermarks(new TimestampedRecordWatermarkEmitter(10));
            return baseFlow;
        }
    
    }
~~~

**3 Transformation：项目中使用到的Transformation操作**

- map：Takes one element and produces one element(将一种元素转换为另一种元素)

- keyBy：将无限数据流拆分成逻辑分组的数据流，在分组数据流中，任何正在传入的事件的属性都可以被当做key，返回KeyedDataStream
数据流

- window：Window函数按时间或其他条件对现有KeyedStream进行分组，一般在keyBy操作后，分组操作流调用window()，非分组操作流调
用windowAll() (滚动窗口、滑动窗口、会话窗口、全局窗口)

- reduce：返回单个结果值

- 以下使用网络流量在1分钟时间内的聚合去重为例
~~~
    // 分组条件
    String[] keyByFields = {"srcMac","dstMac",
                        "timestamp", "agentId", "inPort", "outPort", "protocol",
                        "srcIp", "dstIp", "srcPort","dstPort","tcpFlag", "srcOrg",
                        "dstOrg", "direction", "ipVersion", "sourceAsNumber", "destinationAsNumber"};
    
    baseFlow.map(record->{
        record.setVxLanPayload(null);
        return record;
    }).keyBy(
            keyByFields
    ).window(
            TumblingEventTimeWindows.of(Time.minutes(1))
    ).reduce(new ReduceFunction<SFlowRecord>() {
        // 聚合去重
        @Override
        public SFlowRecord reduce(SFlowRecord value1, SFlowRecord value2) throws Exception {
            value1.setTrafficBytes(value1.getTrafficBytes() + value2.getTrafficBytes());
            value1.setTrafficPackets(value1.getTrafficPackets() + value2.getTrafficPackets());
            return value1;
        }
    })
~~~

- union：union函数将两个或多个数据流结合在一起，这样就可以并行地组合数据流，如果将一个流与自身组合，它会输出每个记录两次

- apply：Applies a general function to the window as a whole，apply用于window后的数据聚合

- 以分别将指定ip和port端口的入流量数据流和出流量数据流统计为ip和port的流量为例
~~~
    // 分组条件为相同ip(agentId)和端口(port)
    String[] keyByFields = {"agentId", "port"};
    // 把相同端口出入流量统计到一条记录上
    inStream.union(
            outStream
    ).keyBy(
            keyByFields
    ).window(
            TumblingEventTimeWindows.of(Time.minutes(2))
    ).apply(new WindowFunction<SimpleSFlowRecord, SimpleSFlowRecord, Tuple, TimeWindow>() {
        @Override
        public void apply(Tuple tuple, TimeWindow timeWindow, Iterable<SimpleSFlowRecord> iterable, Collector<SimpleSFlowRecord> collector) throws Exception {
            SimpleSFlowRecord temp = new SimpleSFlowRecord();
            temp.setTimestamp(timeWindow.getEnd()/1000);
            long outTB = 0, outTP = 0;
            long inTB = 0, inTP = 0;
            int portValue = 0;
            String agentIdValue = "";
            for(SimpleSFlowRecord simpleSFlowRecord : iterable){
                agentIdValue = simpleSFlowRecord.getAgentId();
                portValue = simpleSFlowRecord.getPort();
                inTB += simpleSFlowRecord.getInTrafficBytes();
                inTP += simpleSFlowRecord.getInTrafficPackets();
                outTB += simpleSFlowRecord.getOutTrafficBytes();
                outTP += simpleSFlowRecord.getOutTrafficPackets();
            }
            temp.setInTrafficPackets(inTP);
            temp.setInTrafficBytes(inTB);
            temp.setAgentId(agentIdValue);
            temp.setPort(portValue);
            temp.setOutTrafficBytes(outTB);
            temp.setOutTrafficPackets(outTP);
            collector.collect(temp);
        }
    })
~~~

- flatmap：相比于map的一对一关系，flatmap是一对多(多可以是0,1,2...)

~~~
    DataStream<VLinkSFlowRecord> vlinkStream = detailedStream.flatMap(
            new FlatMapFunction<SFlowRecord, VLinkSFlowRecord>() {
                @Override
                public void flatMap(SFlowRecord sFlowRecord, Collector<VLinkSFlowRecord> collector) throws Exception {
                    for (VLinkPojo vLinkPojo : sFlowRecord.getvLinks()) {
                        VLinkSFlowRecord vLinkSFlowRecord = new VLinkSFlowRecord();
                        /*
                        ...
                        */
                    }
                }
            }
    )
~~~

- filter：过滤函数，符合条件则通过否则被过滤掉

**4 Sink：数据写入elasticsearch**



**5 参考**

- 5.1 flink入门介绍：https://juejin.im/post/5c4f16dbe51d454f342fb7e7#heading-7
- 5.2 flink进阶介绍：https://mp.weixin.qq.com/s?__biz=MzU3Mzg4OTMyNQ==&mid=2247484725&idx=1&sn=c9e53e3df449eb556c615baab87e7d00&chksm=fd3b8b77ca4c0261ad2c2467220a5404733ced62de765f30ca98d55049f79b1c0730f2264e2e&mpshare=1&scene=1&srcid=&sharer_sharetime=1568607570756&sharer_shareid=c9b54d112e2bd5a8d9e7374b0eda6790&key=e16964c072f0d5a7e8bd1ec53b2e2eddec1d2233246e6287bec939811d0f30771c2c54cdceceec21e4e93d1b248ba3d48fe7c9aff2bf5f2d14de4e3676ef21b26b176d5a601a7c20d59e20eade152497&ascene=1&uin=MTE1MzU5ODQxNA%3D%3D&devicetype=Windows+10&version=62060833&lang=zh_CN&pass_ticket=3tDpvo676H%2FKcVPLYOQLjP%2BEggt%2FSodstQ58Gi09k%2FB2oK8tUCw3q9IBVmPXm3pr
- 5.3 flink读出/写入kafka(Apache Kafka Connector)官网地址：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html
  翻译：https://cloud.tencent.com/developer/news/267327
- 5.4 flink使用kafka的实例：https://blog.csdn.net/weixin_41939278/article/details/82803773
- 5.5 flink的transformation官网使用介绍：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/
- 5.6 transformation使用实例：http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/#
- 5.7 transformation的window介绍：https://www.jianshu.com/p/a883262241ef